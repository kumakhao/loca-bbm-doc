\chapter{Grundlagen}
\label{chap:grundlagen}

\section{Der Roboter}
\label{derroboter}
An dieser Stelle soll kurz beschrieben werden, wie die Roboter (siehe Abbildungen~\ref{fig:RobotAndGuests}, \ref{fig:RobotAndLightwall} und \ref{fig:RobotCloseUp}), die \ac{BBM} einsetzt, aufgebaut sind. Sie dienen als Grundlage für die Entwicklung der Simulation. Sie besitzen zwei angetriebene Räder und mindestens ein weiteres Stützrad, welches frei drehbar ist. Die Räder können unabhängig voneinander angesteuert werden. Dazu sind pro Rad ein Servomotor mit Motorsteuerung und ein Getriebe mit Übersetzung verbaut.
  \begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{chapter2/RobotNahaufnahme.png}
    \caption[Nahaufnahme eines der Roboter]{Einer der Roboter in Nahaufnahme}
    \label{fig:RobotCloseUp}
  \end{figure}
Zudem ist an jedem Rad ein Drehimpulsgeber angebracht, der die Umdrehungen der Motorwelle misst. Dieser wird im weiteren Verlauf der Arbeit mit Encoder\footnote{Nach der Englischen Bezeichung: Incremental rotary encoder} abgekürzt. Es gibt verschiedene Baugrößen der Roboter, wobei der Antrieb bei allen baugleich ist. Die Räder haben einen Durchmesser von  160\,$mm$, das Getriebe hat ein Übersetzungsverhältnis von 1:14,5 und die Encoder haben eine Auflösung von 2000 Ticks pro Umdrehung. Lediglich der Radstand unterscheidet sich bei den Baugrößen. Für die Simulation wird mit einem Radstand von 700\,$mm$ gearbeitet.

\section{Bühneninstallation}
\label{buehneninstallation}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{chapter2/BuehneVonOben.png}
	\caption[Bühnenmodell]{Ein Modell einer möglichen Bühne}
	\label{fig:buehnenmodell}
\end{figure}
Die für die Ausstellungen benötigten Bühneninstallationen waren zum Zeitpunkt der Arbeit nicht vorhanden. Sie wurden in der Simulation mit\ac{OSG} nachgebildet und sind in der  Abbildung \ref{fig:buehnenmodell} zu sehen. Die Aufnahme ist von schräg oben auf die 12\,x\,12\,$m$ Grundfläche gemacht worden. An drei Seiten stehen so genannte Lichtwände die 3\,$m$ hoch und 8\,$m$ breit sind. In der obersten Zeile wurde mit schwarzer und weißer Textur ein Bit-Muster modelliert.

\section{Lokalisation}
\label{sec:lokalisation}
Die Lokalisation ist eines der Grundprobleme welches beim Einsatz von mobilen Robotern auftritt, und lässt sicht gemäß \cite[S. 193]{Thrun2006} in drei Teilprobleme untergliedern:
\begin{description}
\item[Position Tracking] ist die Verfolgung der Roboterposition und Orientierung bei bekannten Anfangsbedingungen mit Hilfe von Sensordaten. Dabei spielt das Dynamikmodell des Roboters sowie darin modellierte Unsicherheiten eine wichtige Rolle. Bewegt sich der Roboter von einer bekannten Position aus, wird mit dem Dynamikmodell auf Grund der Odometrie seine neue Position geschätzt. Die Unsicherheiten des Modells erzeugen dabei eine Wahrscheinlichkeitsverteilung um diese neue Position, in der sich die wahre Position befinden sollte. Ohne Messungen von weiteren Sensoren, die Rückschlüsse auf die Umgebung erlauben, würde die Positionsschätzung mit der Zeit immer ungenauer. Mit Hilfe eines Messmodells lässt sich beurteilen, ob eine Messung an einer bestimmten Position wahrscheinlich erscheint oder nicht. Dadurch lässt sich die Wahrscheinlichkeitsverteilung der Position nach einer Bewegung durch eine Messung wieder auf den Bereich, in dem der Messwert des Sensors am wahrscheinlichsten ist, eingrenzen.
\item[Global Localization] ist das Finden der Anfangsposition des Roboters unter allen möglichen Posen die im Szenario vorkommen können. Im Vergleich zum \textit{Position Tracking}, bei dem es genügte, die Unsicherheit um die geschätzte neue Pose zu berücksichtigen, umfasst hier der Raum möglicher Posen ein erheblich größeres Volumen. Ein Ansatz wäre, alle möglichen Posen mit der selben Wahrscheinlichkeit anzunehmen, und mit den ersten Messungen und dem Messmodell diese auf Bereiche einzugrenzen, in denen diese Messungen mit hoher Wahrscheinlichkeit auftreten. Dies ist jedoch sehr viel aufwändiger als das Position Tracking und kann bei einem großen Zustandsraum sehr lange Dauern.
\item[Kidnapped Robot Problem] ist eine verschärfte Variante des \textit{Global Localization} Problems. Dabei geht man davon aus, dass sich der Roboter spontan an einem anderen Ort aufhält als vom Roboter angenommen. Nun müsste die Lokalisation des Roboters wieder im gesamten möglichen Raum erfolgen, nur kann der Roboter diesen Zustand nicht feststellen. Um dieses Problem lösen zu können, müsste ein Algorithmus, parallel zum Position Tracking, den gesamten Zustandsraum prüfen. Bei einigen Anwendungen würde dies zu lange dauern. So wäre es auch denkbar, dass nicht der gesamte Zustandsraum für jede Messung durchsucht wird, sondern nur Teile davon. Entweder geschähe dies systematisch, so dass nach einer gewissen Zeit ebenfalls der gesamte Zustandsraum durchsucht ist, oder durch zufällige Ziehungen aus dem Gesamtvolumen des Zustandsraums, so dass eine Wahrscheinlichkeit besteht, den wahren Zustand zu treffen. 
\end{description}

\subsection{Übersicht gängiger Verfahren}
\label{subsec:uebersichtVerfahren}
Die meisten Lösungsansätze verwenden eine Variante des Bayes-Filters\footnote{in \cite[S. 26]{Thrun2006} beschrieben}. Abhängig vom Einsatzumfeld und der verwendeten Sensoren gibt es jedoch noch andere Verfahren.

In \cite{seco2009survey} zum Beispiel werden, neben dem Bayes-Filter, drei weitere genannt: \textit{Geometry-Based Methods}, \textit{Minimization of the cost function} und \textit{Fingerprint Methods} die sich in Gebäuden, unter Nutzung von Elektromagnetischen Signale, verwenden lassen. Je nach Sensorausstattung werden verschiedene Karten der Umgebung verwendet, auf denen sich der Roboter durch Auswertung von Sensormessungen lokalisieren muss. Dabei gibt es vorher angefertigte, statische Karten, die dem Roboter bereits zur Verfügung stehen, als auch Karten, die der Roboter selbstständig während der Lokalisation erstellen muss. Letzteres wird als \ac{SLAM} Problem bezeichnet, zu dem es bereits viele Veröffentlichungen gibt (u.a.) \cite[S. 309]{Thrun2006}, \cite{Thrun:2002:PFR:2073876.2073937}, \cite[S. 229]{Hertzberg2012}.

Der in Kapitel \ref{chap:einleitung} erwähnte Partikelfilter ist eine Variante des Bayes-Filters. Er ist auch als \ac{MCL} bekannt. Alternativ dazu existiert der Kalman Filter, der mit Normalverteilungen die Wahrscheinlichkeiten abbildet. Er ist in seiner Anwendbarkeit auf lineare Probleme beschränkt oder auf Linearisierung angewiesen. Allerdings ist er eines der ältesten Verfahren und damit am besten untersucht. In \cite{HaXInte2012} wird er bei einem Lokalisationsproblem verwendet. Weiterhin existiert noch so genannte Grid-Based-Filter oder Histogram Filter. Diese teilen den Zustandsraum in so genannte Grids auf, in denen ein Wahrscheinlichkeitswert jeweils den Zustand der Region abbildet. Für bestimmte Probleme mit nur zwei Zuständen kann ein Binary Bayes Filter zum Einsatz kommen. Da in dieser Arbeit ein Partikelfilter verwendet wird, soll seine Funktionsweise erläutert werden.

\subsection{Der Partikelfilter}
\label{subsec:partikelfilter}
\subsubsection*{Zustandsraum}
Der Partikelfilter bildet die Wahrscheinlichkeitsverteilung des vermuteten Zustandes durch eine diskrete Menge von so genannten Partikeln ab. Ein Partikel ist dabei ein Punkt im Zustandsraum zu dem noch ein Gewichtungswert zugeordnet ist. Angenommen der Zustand wäre die Position in einem dreidimensionalen Koordinatensystem, so wäre der Zustandsraum alle möglichen Punkte in diesem Koordinatensystem. Ein Partikel in diesem Raum hätte also vier Attribute: einen X-Wert, einen Y-Wert, einen Z-Wert und einen Gewichtungswert. Im Grundzustand ist der Gewichtungswert bei allen Partikeln gleich und die räumliche Verteilung oder Häufung repräsentiert die Wahrscheinlichkeitsverteilung des vermuteten Zustandes (hier die Position im Koordinatensystem). Häuften sich die Partikel um eine Position, so wäre bei einer zufälligen Ziehung aus den Partikeln die Wahrscheinlichkeit höher, ein Partikel von dieser Position zu ziehen. Mittelt man nun über alle Partikel im Zustandsraum, so erhält man eine Position, die dem vermuteten Zustand entspricht. Dabei ist die Varianz über alle Partikel ein Maß dafür, wie zuverlässig diese Position ist.

\subsubsection*{Dynamikmodell}
Partikelfilter nutzen für die Zustandsschätzung ein Dynamikmodell. In diesem Modell wird abgebildet, wie sich der Zustand über die Zeit verändert, z.B. durch Physikalische Gesetzmäßigkeiten oder durch eine Hilfsgröße die sich messen lässt (z.B. zurückgelegter Weg). Damit erlaubt das Dynamikmodell eine Prognose über den nächsten Zustand abzugeben. Um das Beispiel weiter zu führen stelle man sich vor, man wolle die Position eines Satelliten, der um die Erde kreist, bestimmten. Dabei soll lediglich die Position im Raum und nicht seine Orientierung betrachtet werden. Der Zustandsraum besteht damit aus X, Y, und Z-Koordinaten. Für das Dynamikmodell könnten hier die Keplerschen Gesetze zur Bahnberechnung verwendet werden. Allerdings ist es dafür nötig, die Geschwindigkeit des Objektes zu kennen. Dazu wird der Zustandsraum um die Geschwindigkeiten in X, Y und Z-Richtung erweitert. Nun ist es bei einem gegebenen Zustand möglich, einen Folgezustand nach einer verstrichenen Zeit zu errechnen. Diese Prognose wird allerdings mit der Zeit immer ungenauer, denn in der Realität gibt es immer Störgrößen, die sich nicht vorhersagen lassen. Häufig jedoch lassen sich Aussagen über die Art und Stärke der Störung machen. Dieses Wissen kann in Form von einer Wahrscheinlichkeitsverteilung um die Prognose verwendet werden. Bei dem Satelliten wäre als Störung der Einfluss von Sonnenwind denkbar, der ihn von der Sonne weg beschleunigt. Weiß man wie stark der Einfluss werden kann, könnte man ihn als statistische Größe in die Prognose einfließen lassen. 

\subsubsection*{Messmodell}
Das Messmodell wird benötigt, um entscheiden zu können, wie plausibel eine Messung bei gegebenem Zustand (Beispiel: die Position) ist. Ein Sensor, über den das System verfügt, liefert einen Messwert. Mit dem Messmodell kann zu einem beliebigen Zustand eine Aussage darüber gemacht werden, wie wahrscheinlich es wäre, diesen Messwert zu messen. Im Beispiel des Satelliten könnte der Abstand zu einer Bodenstation gemessen werden. Im Messmodell würde dann aus einem gegebenen Zustand, also Position und Geschwindigkeit, zusammen mit weiteren Informationen, wie Position der Bodenstation auf der Erde sowie Datum und Uhrzeit der Messung errechnet werden wie groß der Abstand sein müsste. Je stärker dieser erwartete Abstand von dem gemessenen abweicht, um so unwahrscheinlicher ist die Messung bei diesem Zustand (Position und Geschwindigkeit). Zusätzlich ist jeder Messwert mit einer Ungenauigkeit behaftet, die vom Messprinzip und Sensortyp abhängt. Aber auch diese lässt sich statistisch angeben und auf den Messwert aufschlagen, bevor er ins Messmodell gegeben wird. So lassen sich Zustände im Zustandsraum hervorheben, für die eine gegebene Messung wahrscheinlich sind.

\subsubsection*{Angewendet auf den Filter}
Wenn der Zustandsraum alle möglichen Zustände eines Systems beschreibt, so sind die Partikel des Partikelfilters eine Untermenge davon, die in ihrer räumlichen Häufung den tatsächlichen Zustand des Systems beschreiben. Während des Betriebs gibt es zwei Ereignisse auf die der Filter reagiert:

Das \textbf{Dynamik-Update} berechnet auf Grundlage des Dynamikmodells eine neue Prognose. Konkret wird im Partikelfilter dafür zu jedem Partikel eine eigene Prognose gestellt. Dabei wird eventuelles Systemrauschen durch eine Ziehung pro Partikel aus dessen Wahrscheinlichkeitsverteilung (häufig Gaußverteilung) abgebildet. Die Prognosen aller Partikel bilden dann den neuen Systemzustand.

Der \textbf{Mess-Update} Aufruf verarbeitet eine Messung mit Hilfe des Messmodells. Dabei wird für jedes Partikel, und damit den Zustand den es repräsentiert, geprüft wie wahrscheinlich eine solche Messung wäre. Bei einer hohen Wahrscheinlichkeit, wird der Gewichtungswert im Partikel größer. Bei geringer Wahrscheinlichkeit für eine solche Messung sinkt der Gewichtungswert. Nun repräsentieren diese Gewichte der Partikel den neuen Systemzustand.

Ein \textbf{Resampling} überführt die Information in den Partikelgewichten wieder in die Partikelhäufung. Es wird normalerweise nach jedem Mess-Update vorgenommen. Dies geschieht durch Ziehen von Partikeln aus der alten Partikelmenge mit einer Wahrscheinlichkeit die proportional zum Gewicht eines Partikels ist. Somit werden Partikel, die ein hohes Gewicht hatten, häufiger gezogen als jene, die nur ein geringes Gewicht hatten. Damit "überleben"  das Resampling die Partikel, deren Zustand von einer vorgenommenen Messung bestätigt wird. Nach der Neuziehung wird das Gesamtpartikelgewicht wieder auf 1 normalisiert und alle Partikel erhalten das selbe Gewicht. In \cite[S. 108]{Thrun2006} wird ein Problem beim Resampling beschrieben, welches bei unabhängigen Ziehungen auftritt. Es wird \emph{variance reduction} genannt und führt dazu, dass Partikel mit geringem Gewicht durch Zufall unter proportional oft oder gar nicht gezogen werden. Dies bedeutet einen Informationsverlust, der sich mit wiederholtem Resampling im Extremfall auf nur einen einzigen Zustand reduziert. Um dem entgegenzuwirken, sollte das Resampling nur durchgeführt werden, wenn sich die Gewichte in den Partikeln verändert haben - beispielsweise nach einer Messung. Ist aber davon auszugehen, dass sich am Zustand nichts ändern kann (z.B. weil der Roboter stillsteht), sollten weder Messungen auswerten, noch ein Resampling durchführen. Zudem sollte systematisches Ziehen anstelle von unabhängigen Ziehungen eingesetzt werden. Hierbei wird ein Partikel zufällig ausgewählt und die übrigen systematisch anhand ihrer Gewichtung ermittelt.

\section{Bildverarbeitung}
\label{sec:bildverarbeitung}
Eine wichtige Grundlage für das Verständnis der Funktion des entwickelten Verfahrens ist das Zustandekommen der Bilder in der am Roboter montierten Kamera. Diese werden als Messung dienen und daher soll an dieser Stelle kurz darauf eingegangen werden, wie sich eine Kamera formal beschreiben lässt.

Das Lochkamera-Modell wird häufig zur Veranschaulichung verwendet. Es vereinfacht viele physikalische Effekte, ist aber eine gute Näherung für die meisten Anwendungen. Für eine detaillierte Beschreibung wären \cite[S. 203]{JaehDigi2005} oder \cite[S. 47]{ToenGrun2005} zu empfehlen. Für das hier vorgestellte Verfahren ist es jedoch wichtig, das Konzept dahinter zu verstehen:

Die Kamera projiziert im Wesentlichen Punkte aus dem Raum (3D) auf die Bildebene (2D). Um nun bei gegebenen Koordinaten eines 3D-Punktes in Weltkoordinaten den passenden Punkt auf der Bildebene berechnen zu können, ist die Kenntnis verschiedener Parameter erforderlich. Dabei trennt man in extrinsische und intrinsische Parameter. Unter den extrinsichen Kameraparametern versteht man die Pose der Kamera in Weltkoordinaten, also wo sich sie befindet und wie sie ausgerichtet ist. Dies wird in einer Transformationsmatrix ausgedrückt, mit der sich Punkte zwischen dem Kamera- und Weltkoordinatensystem  transformieren lassen.

Die intrinsischen Parameter sind nötig, um 3D-Punkte (in Kamerakoordinaten) auf die Bildebene und in Pixelkoordinaten zu projizieren. Sie beinhalten neben der Brennweite der Kamera, auch die Koordinaten, wo die optische Achse auf den Bildsensor trifft. Um die auf den Bildsensor projizierten Punkte auch in Pixeln angeben zu können, gibt es einen Skalierungsfaktor.

Bei realen Kameras muss in der Regel eine Kalibrierung durchgeführt werden um diese Parameter zu bestimmen. Dabei können die intrinsischen Parameter gespeichert und wieder verwendet werden, wenn sich die Brennweite nicht ändert (kein optischer Zoom). Die extrinsischen Parameter müssen neu bestimmt werden, sobald sich die Kamera bewegt. Alternativ muss die Bewegung bekannt sein, um sie in die Transformationsmatrix einfließen zu lassen. 


