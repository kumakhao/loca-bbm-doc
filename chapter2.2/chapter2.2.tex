\chapter{Die Simulationssoftware}
\label{chap:simsoftware}
Die Simulationssoftware und der Lokalisationsalgorithmus die im Rahmen dieser Arbeit entwickelt wurden, sind in der Programmiersprache C++ geschrieben. Als Entwicklungsumgebung wurde Eclipse mit den \ac{CDT} auf einem Linux\footnote{Ubuntu 12.04 LTS}  Betriebssystem verwendet.


Grundlage der Simulation ist das 3D Grafiktoolkit \ac{OSG}\footnote{\url{http://www.openscenegraph.org/}}. Mit dieser Bibliothek lässt sich eine 3D Szene in Form eines Graphen aufbauen und mit einem Viewer darstellen. Um den Ablauf kontrollieren zu können, lässt sich die Render-Schleife manuell aufrufen um jeden Frame einzeln berechnen zu lassen. Dies wurde als Simulationsschritt gewählt in dem alle nötigen Berechnungen durchgeführt werden können.

\section{Koordinatensysteme}
\label{sec:koordinatensysteme}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{chapter2.2/opencvgl_coords.png}
    \caption[Koordinatensysteme in der Simulation]{Koordinatensysteme in der Simulation}
    \label{fig:koordinatensysteme}
\end{figure}
OpenGL und OpenCV definieren die Koordinatensysteme für eine Kamera sehr unterschiedlich. Um die Pose der Kamera aus der Simulation (OpenGL Koordinatensystem) in die extrinsischen Kamera Parameter zu transformieren, wie OpenCV sie verwendet, muss bekannt sein wie diese beiden Systeme zum Weltkoordinatensystem in der Simulation orientiert sind. In Abbildung \ref{fig:koordinatensysteme} sind alle drei Koordinatensysteme eingezeichnet. 

In \textbf{OpenCV} ist die Kamera in die positive z-Achse gerichtet und die y-Achse weist auf den unteren Bildrand. Mit der x-Achse Richtung des rechten Bildrandes, ergibt sich ein Rechtssystem. Sein Ursprung liegt in der Bildmitte so, dass die z-Achse durch den Brennpunkt verläuft.

\textbf{OpenGL} hat die Blickrichtung der Kamera in negativer z-Achse festgelegt. Die y-Achse weist in Richtung des oberen Bildrandes und mit der x-Achse Richtung des rechten Bildrandes, ergibt sich ebenfalls ein Rechtssystem. Sein Ursprung liegt in der Bildmitte.

Im \textbf{Bühnenkoordinatensystem} der Simulation liegen y- und x-Achse in der Ebene der Grundfläche. Die x-Achse weist dabei von der Bühnenseite ohne Lichtwand weg und die z-Achse weist als Normale der Grundfläche nach Oben. Damit ergibt sich ebenfalls ein Rechtssystem, dessen Ursprung die Mitte der Grundfläche ist.
\section{Die Szene}
\label{sec:dieSzene}
Die Szene in der Simulation ist aus mehreren Komponenten aufgebaut, die im Folgenden näher beschrieben werden. Dabei wird ein Vergleich zu den echten Elementen auf der Bühne gezogen, um zu erläutern wie deren Attribute in der Simulation abgebildet werden können. In Abbildung \ref{fig:bildderszene} kann man alle Elemente der Szene erkennen. 

\paragraph*{Die Grundfläche (1)} der Bühne misst 12 x 12 m. Sie wird als einfach weiße Fläche in der Szene dargestellt. Da die Bildverarbeitung nur auf den Oberen Teil des Bildes beschränkt ist, spielen Farbe und Helligkeit keine Rolle bei der Erkennung des Musters im Bild.
 
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{chapter2.2/bildderszene.png}
    \caption[Simulierte Szene]{Simulierte Szene}
    \label{fig:bildderszene}
\end{figure}


\paragraph*{Die Lichtwände (2,3)} sind zu drei Seiten der Grundfläche aufgestellt. Sie messen 3m in der Höhe und 8m  in der Länge. Wie bereits in Abschnitt \ref{sec:bitmuster} beschrieben soll das Muster in der obersten Zeile der Lichtwände dargestellt werden. Hierzu können verschiedene Texturen geladen werden die das Bitmuster enthalten. In der Realität sind die Lichtwände auf der Bühne aus 1 x 1 Meter großen Segmenten zusammen gesetzt, dies wird in der Simulation nicht abgebildet. Es wäre jedoch denkbar, Abstandsänderungen zwischen den Segmenten in den Aufbau der Texturen einfließen zu lassen. Auf jedem Segment sind 8 x 8 Bits unter gebracht. Das macht eine Kantenlänge von 125 mm bei jedem dieser Pixel und 64 Pixel über die ganze Länge einer Wand. Ein solches Segment besteht aus einer milchigen Kunststoffplatte, die auf eine Struktur geschraubt wurde welche für jedes Pixel ein Leuchtmittel vorsieht. Dessen Helligkeit lässt sich einstellen, wäre aber für das Bitmuster auf die Zustände: dunkel(ausgeschaltet) oder hell(ein, mit größter Helligkeit) einzustellen. Die Plexiglasplatte wird also pro Pixel von hinten durchleuchtet. Dies führt dazu, dass die Helligkeitsverteilung in einem beleuchteten Pixel inhomogen ist. In der Mitte ist die größte Helligkeit, während sie radial nach außen abnimmt. Die Ecken der Pixel sind die dunkelsten Stellen. Auf Abbildung \ref{fig:RobotAndLightwall} auf Seite \pageref{fig:RobotAndLightwall} ist dieser Effekt zu erkennen. Er ist stärker, wenn die Leuchtmittel mit niedriger Helligkeit betrieben werden. Bei hoher Helligkeit ist der Effekt noch wahrnehmbar, aber nicht mehr so ausgeprägt. Betrachtet man zwei gleich angesteuerte Pixel ist zu erkennen, dass die Lichtfarbe und Helligkeit leicht von Pixel zu Pixel variiert. All diese Effekte können in der Simulation durch verändern der Texturen abgebildet werden.

\paragraph*{Der Roboter (4)} wird duch einen weißen Würfel dargestellt. Die Kameras sind so angebracht, dass der Roboter selbst nicht im Bild zu sehen ist. Der Würfel ist für den Bediener gedacht, der die Szene aus einem Blickwinkel hinter dem Roboter steuert. Abbildung \ref{fig:bildderszene} zeigt ein Bild, welches aus diesem Blickwinkel aufgenommen worden ist. Der Roboter lässt sich mit den Tasten W,~A,~S,~D (vorwärts, links, rückwärts, rechts) oder mit einem Pad\footnote{PlayStation 3 Wireless Sixaxis Controller} mit Analogsticks steuern. Mit der Tastensteuerung ist nur eine feste Geschwindigkeit möglich. Die Steuerbefehle für den Roboter werden als Geschwindigkeit (gerade aus) und Drehrate interpretiert. Diese führen dann, in jedem Simulationsschritt zu einer Positionsänderung. Es wird erst eine Drehung um die Hälfte der Winkeländerung durchgeführt. Anschließend wird in die neuer Richtung geradeaus die Streckenänderung zurück gelegt. Mit einer zweiten Drehung bis zur vollen Winkeländerung wird die Bewegung abgeschlossen. Die Position des Roboters wird relativ zum Bühnenkoordinatensystem in x- und y-Werten ausgedrückt. Im Roboterkoordinatensystem zeigt die x-Achse immer in Fahrtrichtung des Roboters, die z-Achse zeigt nach oben und mit der y-Achse ergibt sich ein Rechtssystem. Der Ursprung dieses Systems ist mittig zwischen den Rädern auf Bodenhöhe. Die Orientierung des Robotermodells wird als Winkel zwischen den x-Achsen von Roboter- und Bühnenkoordinatensystem ausgedrückt.

\paragraph*{Besucher (5)} werden mit 400\,x\,400\,x\,1800\,$mm$ Quadern dargestellt. Damit sie für den Bediener besser in der Szene zu erkennen sind, wurden sie grün gefärbt. Sie bewegen sich nicht, sondern stehen an vorher festgelegten Positionen. Vor einer Simulation wird die Anzahl der Besucher eingestellt, die auf der Bühne stehen sollen. Dabei gibt es 25 mögliche Positionen aus denen dann die gewünschte Anzahl zufällig gezogen wird. Sie sollen mögliche Verdeckungen im Bildbereich des Roboters darstellen. Damit soll untersucht werden, wie gut der Algorithmus mit teilweise verdeckten Code-Bereichen umgehen kann. 

\paragraph*{Partikel (6)} werden zur Veranschaulichung und zu debugging Zwecken visualisiert. Sie werden durch rote kleine spitze Dreiecke dargestellt. Der spitze Winkel zeigt dabei die Orientierung an. Sie befinden sich nur auf dem Boden und beeinträchtigen die Bildverarbeitung deshalb nicht.

\section{Messen in der Simulation}
\label{sec:MessenInDerSimulation}
Die Simulation soll, neben der Visualisierung, Messungen liefern um den Lokalisationsalgorithmus testen zu können. Im Gegensatz zu Messungen an realen Experimenten gibt es in der Simulation die Möglichkeit, alle das Messergebnis beeinflussenden Faktoren festlegen zu können. Beispielsweise können systematische Abweichungen bei Messungen in der Simulation definiert werden. Ähnlich verhält es sich mit statistischen Abweichungen. Es lassen sich also schnell die Messunsicherheiten der simulierten Vorgänge anpassen. Beim Debugging und Funktionstest des Lokalisationsalgorythmus wurde die Unsicherheit zum Beispiel zeitweise entfernt.

\subsection{Messwerte der Encoder}
\label{subsec:encoderWerteInSim}
Wie beim Robotermodell in Abschnitt \ref{sec:dieSzene} beschrieben, wird in der Simulation dessen x/y-Koordinate sowie der Winkel zur x-Achse als Repräsentation der Pose verwendet. Aus den Positions- ($ \Delta s $) und Orientierungsänderungen ($ \Delta \psi $), in jedem Simulationsschritt, werden Drehwinkeländerungen ($ \Delta \alpha_{r/l} $) der beiden Räder berechnet:
\begin{align}
\Delta \alpha_{rechts} &= \frac{\Delta s + \Delta \psi \cdot D_r}{R_{r}} \\ 
\Delta \alpha_{links} &= \frac{\Delta s - \Delta \psi \cdot D_l}{R_{l}} 
\end{align}
mit Abstand $ D_{r/l} $ des Rades von der Mitte der Achse und Radradius $ R_{rad} $. Die Drehwinkeländerungen werden aufsummiert um die Radstellungen zu speichern:
\begin{align}
\alpha = \alpha + \Delta \alpha
\end{align}
Aus diesen Winkelstellungen\footnote{Die Winkelstellung der Räder wird nur numerisch simuliert und ist am Modell nicht sichtbar.}  der Räder wird ein Wert für die Encoder abgeleitet und auf ganze Impulse gerundet:
\begin{align}
I_{rechts} &= \alpha_{rechts} \cdot \frac{g\ \gamma}{2\pi}\\
I_{links} &= \alpha_{links} \cdot \frac{g\ \gamma}{2\pi}
\end{align}
mit Getriebeübersetzung: $ g $ und Encoder-Auflösung(pro Umdrehung): $ \gamma $

\subsection{Messabweichung der Encoder}
\label{subsec:encoderFehlerInSim}
Es können drei verschiedene Typen von Messabweichungen simuliert werden:

Eine \textbf{systematische Abweichung} durch einen ungenau bekannten Radradius. Dies führt zu einem sich akkumulierenden Fehler in der Gefahrenen Strecke und der Orientierung. In der Simulation wurde diese Art Unsicherheit durch einen veränderten Radradius $R_{r/l}$ erzeugt. Für beide Räder wird vor jedem Simulationslauf in einem Fehlerintervall der Radius leicht verändert.
\begin{equation}
R_{r/l}=R_{Maß}+R_{Maß}\cdot \sigma_{R} \cdot RandomGaussian()
\end{equation}
Mit $\sigma_{R}$ lässt sich einstellen, wie stark die Abweichung ist. Je stärker die Radien sich unterscheiden, desto stärker scheint der Roboter bei geradeaus Fahrt eine lichte Kurve zu fahren. Diese Art Messabweichung ist über die Dauer eine Simulation konstant, aber für jeden Simulationslauf zufällig.

Eine \textbf{systematische Abweichung} die nur bei Drehungen auftritt. Dies ist typischerweise die dominante Fehlerquelle bei radgetriebenen Robotern. Bei einer Drehung ist der Auflagepunkt der Räder nicht bekannt, da der innen liegende Teil des Rades sich langsamer drehen müsste als der äußere. Dies führt bei einer Drehung dazu, dass der Abstand des Rades zur Drehachse nicht genau bekannt ist. In der Simulation wird daher dieser Abstand $D_{r/l}$ über die Breite $W_{r/l}$ der Räder zufällig bestimmt.
\begin{equation}
D_{r/l}=\frac{K}{2}+W_{r/l}\cdot 0,5\cdot RandomGaussian()
\end{equation}
mit Spurbreite $K$ von Radmitte zu Radmitte.

Dieser Abstand müsste für jede Drehung neu bestimmt werden. Zum Testen der Lokalisation reicht es aber aus, wenn dies nur einmal zu Beginn einer Simulation geschieht. Dadurch treten bei Drehungen Messabweichungen auf, die zwar über einen Simulationslauf konstant und damit systematisch sind, aber die Lokalisierung genauso erschweren wie zufällige Abstände während der Laufzeit.

Eine \textbf{zufällige Abweichung} bei den Encoder-Messwerten soll ein mögliches Spiel in Kupplung oder Getriebe simulieren. Dabei wird zu dem akkumulierten Encoder-Wert eines Rades ein Gaußsches Rauschen $err$ addiert, welches proportional zur letzten Winkeländerung $\Delta\psi$ ist. Ist diese bei Stillstand gleich Null, so wird nur ein schwaches Grundrauschen addiert.
\begin{equation}
err=\left(\Delta\psi + RobotMaxSpeed\cdot 0.01\right)\cdot psi2I \cdot \sigma_I \cdot RandomGaussian()
\end{equation}
mit einem Umrechnungsfaktor $psi2I$ der hier zur Abkürzung verwendet wird. Das $\sigma_I$ ist ein Koeffizient, mit dem sich die Stärke des Rauschens einstellen lässt und $(RobotMaxSpeed\,\cdot\,0.01)$ ist für eine Grundrauschen zuständig, das proportional zur eingestellten Roboter-Maximalgeschwindigkeit ist. 
\subsection{Bilder der Kamera}
\label{subsec:bilderderkamera}
Die Kamera des Roboters wird in der Simulation nicht als Modell angezeigt. Sondern lediglich durch ein Objekt der Klasse osgViewer::View implementiert. Um einer echten Kamera möglichst nah zu kommen, wurde die Auflösung sowie der Öffnungswinkel der am Roboter verbauten Kamera in der Simulation nach empfunden. Die Auflösung der Bilder ist mit 1280\,x\,720 Pixeln im 16:9 Format. Der Horizontale Öffnungswinkel beträgt 74\,Grad. Die verwendete OSG Bibliothek setzt auf OpenGL auf und nutzt deshalb auch die dort verwendeten Funktionen zur Perspektivischen Projektion. In Abbildung \ref{fig:OpenGLfrustum} ist zu sehen, wie diese in OpenGL beschrieben wird. Ein Pyramiden Stumpf begrenzt dabei ein Volumen, in dem alle sichtbaren Objekte liegen. Dieser Pyramidenstumpf wird auch \textbf{frustum} genannt. Seine Eigenschaften werden üblicher Weise in einer Projektionsmatrix zusammen gefasst, die sich direkt, oder über Hilfsfunktionen definieren lässt. Die verwendete Funktion benötigt folgende Parameter:
\begin{description}
\item[fovy] Gibt den vertikalen Öffnungswinkel in Grad an.
\item[aspect] Gibt das Verhältnis zwischen horizontalem und vertikalem Öffnungswinkel an. Es sollte gleich dem Seitenverhältnis der Bildauflösung gewählt werden, um keine Verzerrungen zu erzeugen. (Breite/Höhe)
\item[zNear] Gibt den Abstand zwischen Betrachter und Projektionsfläche an. Sie wird auch zum clipping verwendet. (immer positiv)
\item[zFar] Gibt den Abstand zur hinteren clipping Ebene an. Alles was weiter als diese Ebene entfernt ist, wird nicht mit auf das Bild projiziert. (immer positiv)
\end{description}
\begin{figure}[h]
	\center
	\includegraphics[width=1.0\textwidth]{chapter2.2/800px-Field_of_view_angle_in_view_frustum.png}
	\caption[OpenGL frustum]{OpenGL beschreibt die perspektivische Projektion mit Hilfe eines \textbf{frustum}}
	\label{fig:OpenGLfrustum}
\end{figure}
OpenGL projiziert zunächst auf eine normalisierte Bildebene. Aus dieser normalisierten Darstellung muss mittels einer Viewporttransformation in die Bildschirmkoordinaten (Pixel) umgerechnet werden. Der Viewport ist dabei der Bildschirmbereich, auf den das gerenderte Bild angezeigt wird. Mit OSG lässt sich der Viewport für eine Kamera mit der gewünschten Auflösung (Höhe, Breite) und einem Ursprung setzten. Die Matrix für die Viewporttransformation heißt in OSG windowMatrix, und wird automatisch aus dem gesetzten Viewport berechnet.

Die Bilder der Roboter Kamera werden nicht auf dem Bildschirm angezeigt, sondern in einen Buffer geschrieben und von dort in ein für OpenCV nutzbares Format kopiert. In dieser Form werden die Bilder auf der Festplatte gespeichert und an die Lokalisation als Messupdate gegeben.

\section{Einstellungen}
\label{sec:einstellungenSim}
In der Simulationssoftware lassen sich bestimmte Parameter verändern, um ihr Verhalten zu definieren. Hier soll kurz zusammen gefasst werden welche Parameter es gibt, und was sie beeinflussen. In Klammern steht der voreingestellte Default-Wert.
\begin{description}
\item[picture path](<path>) Die Bilder die während eines Simulationslaufes von der Kamera gemacht werden, werden an diese Stelle gespeichert. Sie sind aufsteigend nummeriert, beginnend mit "0000.jpg"
\item[datafile name]("locaDatafile.txt")Der name der Datei in der die Rohdaten eines Simulationslaufes gespeichert werden. Sie enthält Encoder-Werte, echte Roboterpose, geschätzte Roboterpose, Varianz Roboterpose und den Pfad zum Bild das zu den Messwerten gehört. Die Werte werden in jedem Simulationsschritt geschrieben.
\item[takepicture intervall](3000) Zeit in Millisekunden zwischen zwei Kamerabildern. Dabei wird \textbf{loop~target~time} verwendet um zu bestimmen wie viele Simulationsschritte der Zeitangabe entsprechen.
\item[loop target time](33333) Zeit in Mikrosekunden die einem Simulationsschritt entspricht. Der default wert entspricht 30 Schritte pro Sekunde.
\item[crowd size](0)	Stellt ein wie viele Personen auf der Bühne sein sollen. Mögliche sind 0 bis 23 Personen.
\item[particle visibility ratio](1) Dies stellt ein wie viele Partikel visualisiert werden. Es beeinflusst die Simulation nicht und dient nur dem Bediener um das Partikelverhalten sehen zu können. Der Wert ist als Teiler zu verstehen, 1 bedeutet alle Partikel werden visualisiert während 10 nur ein Zehntel darstellt. 
\item[sys error on](true) Mit diesem Flag kann der Systematische Fehler ein- oder ausgeschaltet werden.
\item[camera picture width](1280) Gibt die Bildauflösung der Simulierten Kamera an.
\item[camera picture height](720) Gibt die Bildauflösung der Simulierten Kamera an.
\item[field of view horizontal](74) Dies stellt den horizontalen Öffnungswinkel der Kamera ein.
\item[blind mode on](false) Dieses Flag deaktiviert zwei Fenster  der Simulation mit denen ein Bediener die Vorgänge beobachten kann. Dies ist sinnvoll, wenn automatisiert viele Simulationsdurchläufe durchgeführt werden sollen.
\item[robParameter.kDistanceLeftWheel](0.35) Abstand linkes Rad zum Robotermitte
\item[robParameter.kDistanceRightWheel](0.35) Abstand rechtes Rad zum Robotermitte
\item[robParameter.kLeftWheelWidth](0.03) Dicke linkes Rad
\item[robParameter.kRightWheelWidth](0.03) Dicke rechtes Rad
\item[robParameter.kRadiusWheelRight](0.08) Rechter Radradius 
\item[robParameter.kRadiusWheelLeft](0.08) Linker Radradius
\item[robParameter.kImpulePerTurn](2000) Encoder-Auflösung pro Umdrehung
\item[robParameter.kTransmissionKoefficent](14.5) Faktor zwischen Raddrehung und Motorwelle
\item[robParameter.kPsiSpeed](0.05) Winkeländerung bei Drehung pro Schritt
\item[robParameter.kSigmaIncrement](0.05) Koeffizient für gaußsches Rauschen der Encoder-Werte
\item[robParameter.kSigmaRadius](0.003) Koeffizient für systematischen Radiusabweichung.
\item[robParameter.kSpeed](0.05) Strecke die pro Simulationsschritt zurückgelegt wird.
\end{description}